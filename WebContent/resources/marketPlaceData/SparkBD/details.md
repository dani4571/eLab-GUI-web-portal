# Big Data Analysis with Spark

### Tools

The following cloud-based programming environment will be provided to each student: 
Python, Python libraries, pre installed Spark, NumPy, SciPy, Matplot libraries, SymPy and Github for submitting project code.

### Recommended Background

Prior Programming in Python and scripting experience is required. You should have some familiarity with command-line too. All exercises will use Python and above mentioned libraries. 

### About the Course

Manipulating big data distributed over a cluster using functional concepts is rampant in industry, and is arguably one of the first widespread industrial uses of functional ideas. This is evidenced by the popularity of MapReduce and Hadoop, and most recently Apache Spark, a fast, in-memory distributed collections framework written in Scala. In this course, we'll see how the data parallel paradigm can be extended to the distributed case, using Spark throughout.

Apache Spark is an advanced and next generation open source Big Data Processing platform. Spark is developed to provide fast processing of large datasets and high performance for a wide range of applications. Spark facilitates inmemory cluster computing which greatly improves the speed of iterative algorithms and interactive data mining work. Spark gives us a comprehensive, unified framework to manage big data processing requirements with a variety of data set and source of data like batch processing and real time sreaming data. With Spark one can run applications in Hadoop cluster to run up to 100 times faster in memory and 10 times faster even when runnig on disk. In addition, it also supports SQL queries, streaming data, machine learning and graph data processing.   

This course is designed to give you the main principles needed to understand Apache Spark to solve some of the big data problems and build your confidence through hands-on experience. Through the course, the audience will be guided with a wide range of core Spark concepts using Python source code examples; all of which are desined to give fundamental & working knowledge. These lessons will help to uderstand and execute the basic programming and data manipulation in Spark. There will be few python examples illustrated and explained to take you to the next level.

### Syllabus

- Introduction to Apache Spark
- Spark fundamentals / basics
- Using Resilient Distributed Datasets (RDDs)
- Parallelized Collections
- RDD Operations
- Data processing with Spark
- Spark SQL
- Spark Streaming
- MLLib (Mahine Learning Library ) 
- Using GraphX
- Simple examples / cases overview
- Big Data Processing examples with Spark
